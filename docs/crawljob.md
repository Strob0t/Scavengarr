# CrawlJob System

The CrawlJob system bridges Torznab search results with JDownloader. When a search
returns results, each result is converted to a `CrawlJob` containing validated
download links in JDownloader's `.crawljob` format.

## How It Works

1. **Search** returns `SearchResult` objects with download links
2. **Link validation** filters out dead/unreachable links (parallel HEAD requests)
3. **CrawlJobFactory** creates a `CrawlJob` with validated links and metadata
4. **CrawlJobRepository** stores the job in cache (TTL: 1 hour)
5. **Torznab XML** includes `<link>` pointing to `/api/v1/download/{job_id}`
6. **Arr apps** (Sonarr/Radarr) request the download URL
7. **Download endpoint** serves the `.crawljob` file

## CrawlJob Entity

```python
@dataclass
class CrawlJob:
    # Core
    job_id: str              # UUID4 (auto-generated)
    text: str                # Newline-separated download links
    package_name: str        # Display name in JDownloader

    # Metadata
    validated_urls: list[str]  # Validated download links
    source_url: str | None     # Original detail page URL
    comment: str | None        # Human-readable description
    created_at: datetime       # Creation timestamp (UTC)
    expires_at: datetime       # Expiration timestamp (UTC)

    # JDownloader behavior flags
    auto_start: BooleanStatus  # TRUE / FALSE / UNSET
    auto_confirm: BooleanStatus
    priority: Priority         # HIGHEST / HIGHER / HIGH / DEFAULT / LOWER
    enabled: BooleanStatus
    # ... and more
```

## `.crawljob` File Format

The serialized format is a key-value property file compatible with JDownloader's
FolderWatch extension:

```
# Generated by Scavengarr
# Job ID: abc123-...
# Created: 2025-01-01T12:00:00+00:00
# Expires: 2025-01-01T13:00:00+00:00

text=https://example.com/download/1
packageName=Iron.Man.2008
comment=Source: https://example.com/movie/1
autoStart=TRUE
autoConfirm=UNSET
forcedStart=UNSET
enabled=TRUE
extractAfterDownload=UNSET
chunks=0
priority=DEFAULT
deepAnalyseEnabled=false
addOfflineLink=true
overwritePackagizerEnabled=false
setBeforePackagizerEnabled=false
```

### Fields

| Field | Description | Default |
|-------|-------------|---------|
| `text` | Download links (newline-separated) | (required) |
| `packageName` | Package display name | `Scavengarr Download` |
| `filename` | Override filename | (none) |
| `downloadFolder` | Custom download directory | (none) |
| `comment` | Description text | (none) |
| `autoStart` | Auto-start download | `TRUE` |
| `autoConfirm` | Skip confirmation | `UNSET` |
| `forcedStart` | Force start | `UNSET` |
| `enabled` | Enable download | `TRUE` |
| `extractAfterDownload` | Auto-extract archives | `UNSET` |
| `chunks` | Max parallel chunks (0 = default) | `0` |
| `priority` | Download priority | `DEFAULT` |
| `deepAnalyseEnabled` | Deep link analysis | `false` |
| `addOfflineLink` | Add even if offline | `true` |
| `extractPasswords` | Archive passwords (JSON array) | (none) |
| `downloadPassword` | Link password | (none) |

## Factory Configuration

The `CrawlJobFactory` is configured at startup with defaults:

| Setting | Default | Description |
|---------|---------|-------------|
| `default_ttl_hours` | `1` | Job expiration time |
| `auto_start` | `true` | JDownloader auto-start |
| `default_priority` | `DEFAULT` | Download priority |

### Field Mapping

| SearchResult Field | CrawlJob Field |
|-------------------|----------------|
| `title` | `package_name` |
| `download_link` | `text`, `validated_urls` |
| `source_url` | `source_url` |
| `release_name` | Used in `comment` |
| `description` | Used in `comment` |
| `size` | Used in `comment` |

## Storage

CrawlJobs are stored via the `CrawlJobRepository` port, currently implemented
as `CacheCrawlJobRepository`:

- **Key:** `crawljob:{job_id}`
- **Value:** Pickle-serialized `CrawlJob`
- **TTL:** 3600 seconds (1 hour)
- **Backend:** Diskcache (SQLite) or Redis

## Download Flow

```
Prowlarr/Radarr                    Scavengarr
     |                                  |
     |  GET /api/v1/download/{job_id}   |
     |--------------------------------->|
     |                                  |-- Lookup in cache
     |                                  |-- Check expiry
     |                                  |-- Serialize to .crawljob
     |  200 OK (application/x-crawljob) |
     |<---------------------------------|
     |                                  |
     |  Save to JDownloader watch dir   |
     |                                  |
```

## Expiration

- CrawlJobs expire after 1 hour (configurable)
- Expired jobs return HTTP 404
- Cache TTL handles automatic cleanup
