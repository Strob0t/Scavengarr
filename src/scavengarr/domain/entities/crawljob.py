"""CrawlJob entity - Represents a JDownloader .crawljob file."""

from __future__ import annotations

import json
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from uuid import uuid4


class BooleanStatus(str, Enum):
    """JDownloader BooleanStatus enum (TRUE/FALSE/UNSET)."""

    TRUE = "TRUE"
    FALSE = "FALSE"
    UNSET = "UNSET"


class Priority(str, Enum):
    """JDownloader download priority."""

    HIGHEST = "HIGHEST"
    HIGHER = "HIGHER"
    HIGH = "HIGH"
    DEFAULT = "DEFAULT"
    LOWER = "LOWER"


@dataclass(frozen=True)
class CrawlJob:
    """Represents a .crawljob file for JDownloader.

    Contains validated download links and metadata for Sonarr/Radarr integration.

    Fields based on JDownloader's CrawlJobStorable format:
    https://github.com/mirror/jdownloader/blob/trunk/src/org/jdownloader/extensions/folderwatchV2/CrawlJobStorable.java

    Core fields:
        job_id: Unique identifier (UUID4).
        text: Download links (newline-separated) - REQUIRED by JDownloader.
        package_name: Display name in JDownloader package list.
        filename: Override filename (optional).
        download_folder: Custom download directory (optional).
        comment: User-visible description.

    Validation metadata:
        validated_urls: List of validated download links (from link validator).
        source_url: Original indexer page URL.
        created_at: Timestamp when job was created.
        expires_at: Expiration timestamp (default: 1 hour after creation).

    JDownloader behavior flags:
        auto_start: Auto-start download when added (TRUE/FALSE/UNSET).
        auto_confirm: Skip confirmation dialogs (TRUE/FALSE/UNSET).
        extract_after_download: Auto-extract archives (TRUE/FALSE/UNSET).
        priority: Download priority (HIGHEST/HIGH/DEFAULT/LOWER).
        chunks: Max parallel download chunks (0 = default).
        deep_analyse_enabled: Enable deep link analysis (bool).
        add_offline_link: Add link even if offline (bool).
        extract_passwords: List of archive passwords.
        download_password: Password for password-protected links.
    """

    # === Core Identification ===
    job_id: str = field(default_factory=lambda: str(uuid4()))
    text: str = ""  # Newline-separated download links (REQUIRED)

    # === Display Metadata ===
    package_name: str = "Scavengarr Download"
    filename: str | None = None
    comment: str | None = None

    # === Validation Metadata (Scavengarr-specific) ===
    validated_urls: list[str] = field(default_factory=list)
    source_url: str | None = None
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    expires_at: datetime = field(
        default_factory=lambda: datetime.now(timezone.utc) + timedelta(hours=1)
    )

    # === Download Configuration ===
    download_folder: str | None = None
    chunks: int = 0  # 0 = use JDownloader default
    priority: Priority = Priority.DEFAULT

    # === Behavior Flags ===
    auto_start: BooleanStatus = BooleanStatus.TRUE
    auto_confirm: BooleanStatus = BooleanStatus.UNSET
    forced_start: BooleanStatus = BooleanStatus.UNSET
    enabled: BooleanStatus = BooleanStatus.TRUE
    extract_after_download: BooleanStatus = BooleanStatus.UNSET

    # === Archive/Security ===
    extract_passwords: list[str] = field(default_factory=list)
    download_password: str | None = None

    # === Advanced Options ===
    deep_analyse_enabled: bool = False
    add_offline_link: bool = True
    overwrite_packagizer_enabled: bool = False
    set_before_packagizer_enabled: bool = False

    def is_expired(self) -> bool:
        """Check if CrawlJob has expired.

        Returns:
            True if current time > expires_at.
        """
        return datetime.now(timezone.utc) > self.expires_at

    def to_crawljob_format(self) -> str:
        """Serialize to JDownloader .crawljob format.

        Format: Key-value pairs (property file format).
        See: https://board.jdownloader.org/showthread.php?t=58281

        Returns:
            String content for .crawljob file.
        """
        lines = [
            "# Generated by Scavengarr",
            f"# Job ID: {self.job_id}",
            f"# Created: {self.created_at.isoformat()}",
            f"# Expires: {self.expires_at.isoformat()}",
            "",
            # === Core Fields ===
            f"text={self.text}",
            f"packageName={self.package_name}",
        ]

        # === Optional Fields ===
        if self.filename:
            lines.append(f"filename={self.filename}")

        if self.download_folder:
            lines.append(f"downloadFolder={self.download_folder}")

        if self.comment:
            lines.append(f"comment={self.comment}")

        # === Behavior Flags ===
        lines.extend(
            [
                f"autoStart={self.auto_start.value}",
                f"autoConfirm={self.auto_confirm.value}",
                f"forcedStart={self.forced_start.value}",
                f"enabled={self.enabled.value}",
                f"extractAfterDownload={self.extract_after_download.value}",
            ]
        )

        # === Download Config ===
        lines.extend(
            [
                f"chunks={self.chunks}",
                f"priority={self.priority.value}",
                f"deepAnalyseEnabled={str(self.deep_analyse_enabled).lower()}",
                f"addOfflineLink={str(self.add_offline_link).lower()}",
                f"overwritePackagizerEnabled={str(self.overwrite_packagizer_enabled).lower()}",
                f"setBeforePackagizerEnabled={str(self.set_before_packagizer_enabled).lower()}",
            ]
        )

        # === Passwords (JSON array format) ===
        if self.extract_passwords:
            lines.append(
                f"extractPasswords="
                f"{json.dumps(self.extract_passwords, separators=(',', ':'))}"
            )

        if self.download_password:
            lines.append(f"downloadPassword={self.download_password}")

        return "\n".join(lines)
