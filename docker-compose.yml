services:
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ai-ollama
  #   #devices:
  #   #  - /dev/kfd
  #   #  - /dev/dri
  #   ports: ["11434:11434"]
  #   volumes:
  #     - ./.devdata/ollama:/root/.ollama
  #   networks: [mcp-network]

  docs-mcp-server:
    image: ghcr.io/arabold/docs-mcp-server:latest
    container_name: ai-docs-mcp-server
    ports:
      - "6280:6280" # Docs MCP für LLM-Context
    volumes:
      - ./:/docs:ro # optional: lokale HTML/Markdown Doku als Quelle
      - ./.devdata/docs_mcp_data:/data # persistente DB/Index
    environment:
      # - OPENAI_API_BASE=http://ollama:11434/v1
      # - OPENAI_API_KEY=ollama
      # - DOCS_MCP_EMBEDDING_MODEL=nomic-embed-text
      - OPENAI_API_BASE=http://192.168.88.21:1234/v1
      - OPENAI_API_KEY=lmstudio
      - DOCS_MCP_EMBEDDING_MODEL=text-embedding-qwen3-embedding-8b
      - DOCS_MCP_TELEMETRY=false
    command: ["--protocol", "http", "--host", "0.0.0.0", "--port", "6280"]
    # depends_on:
    #   - ollama
    networks:
      - mcp-network

  # filesystem-mcp-gw:
  #   image: supercorp/supergateway:latest
  #   container_name: ai-filesystem
  #   command:
  #     - "--stdio"
  #     - "npx"
  #     - "@modelcontextprotocol/server-filesystem"
  #     - "/workspace"
  #     - "--allow-create"
  #     - "--allow-delete"
  #   volumes:
  #     - .:/workspace:rw
  #   ports:
  #     - "8000:8000"
  #   networks:
  #     - mcp-network

  llm-context-sse:
    image: node:20-bookworm-slim
    container_name: ai-llm-context-sse
    working_dir: /workspace
    volumes:
      - .:/workspace:cached
      - ./.devdata/llm_context_state:/state
    environment:
      - HOME=/root
    command:
      - sh
      - -lc
      - >
        apt-get update &&
        apt-get install -y --no-install-recommends curl ca-certificates &&
        rm -rf /var/lib/apt/lists/* &&
        curl -LsSf https://astral.sh/uv/install.sh | sh &&
        export PATH=/root/.local/bin:$PATH &&
        exec npx -y supergateway --stdio "uvx --from llm-context lc-mcp" --outputTransport streamableHttp --stateful --sessionTimeout 600000 --port 9000 --streamableHttpPath /mcp
    ports:
      - "9000:9000"
    networks:
      - mcp-network
    restart: unless-stopped

  playwright-mcp:
    image: mcr.microsoft.com/playwright:v1.57.0-jammy
    container_name: ai-playwright
    ipc: host
    shm_size: "1gb"
    volumes:
      - ./.devdata/playwright-mcp:/config
    command:
      - sh
      - -c
      - |
        npx playwright install --with-deps chromium &&
        exec npx supergateway --stdio "npx @playwright/mcp@latest --headless --no-sandbox --isolated" \
          --outputTransport streamableHttp \
          --stateful \
          --sessionTimeout 600000 \
          --port 8001 \
          --streamableHttpPath /mcp
    ports:
      - "8001:8001"
    networks:
      - mcp-network

  # 3. OBSERVABILITY (OTEL + Jaeger)
  # otel-collector:
  #   image: otel/opentelemetry-collector-contrib:latest
  #   container_name: ai-otel-collector
  #   command: ["--config=/etc/otel-collector-config.yaml"]
  #   volumes:
  #     - ./docker/otel-config.yaml:/etc/otel-collector-config.yaml:ro
  #   ports:
  #     - "4318:4317" # OTLP gRPC
  #     - "4319:4318" # OTLP HTTP
  #     - "8888:8888" # Metrics
  #   networks:
  #     - mcp-network
  #   healthcheck:
  #     test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133/"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: ai-jaeger
    ports:
      - "4317:4317" # OTLP gRPC receiver
      - "14268:14268" # Jaeger collector
      - "16686:16686" # Jaeger UI
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=memory # Für Development; nutze badger/cassandra für Production
    networks:
      - mcp-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:14269/"]
      interval: 30s
      timeout: 10s
      retries: 3

  proxy:
    image: nginx:alpine
    container_name: local-proxy
    ports:
      - "3000:80"
    command:
      - /bin/sh
      - -lc
      - |
        cat > /etc/nginx/conf.d/default.conf <<'EOF'
        server {
          listen 80;

          location = /api/v1/responses {
            rewrite ^ /api/v1/chat/completions break;

            proxy_pass https://premiumize.ai;

            proxy_set_header Authorization $$http_authorization;
            proxy_set_header Host premiumize.ai;

            proxy_buffering off;
            proxy_cache off;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
            proxy_intercept_errors off;
            add_header X-Upstream-Status $$upstream_status always;
          }
        }
        EOF

        exec nginx -g 'daemon off;'

# 5. NETWORK (MCP Communication)
networks:
  mcp-network:
    driver: bridge
    name: scavengarr_mcp
